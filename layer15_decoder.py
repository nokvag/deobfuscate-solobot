#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
layer15_decoder.py
    â€¢ Ğ¸Ñ‰ĞµÑ‚ ĞºĞ¾Ñ€Ñ‚ĞµĞ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´Ğ°   (X,) = (__import__('asyncio'),)
                                        (A, B) = (getattr(...,'foo'), getattr(...,'bar'))
    â€¢ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ ĞºĞ°Ñ€Ñ‚Ñƒ  obf_name â†’ real_name
    â€¢ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¸Ğº Ğ¸ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑĞ¾Ğ²Ğ¾Ğµ Â«rename variableÂ»
    â€¢ Ğ¿Ğ¸ÑˆĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ²  layer15.py
"""
import io, re, tokenize, ast, pathlib, keyword

SRC = pathlib.Path("layer14.py")
DST = pathlib.Path("layer15.py")
code = SRC.read_text(encoding="utf-8")

# â”€â”€ 1. Ğ³Ñ€ÑƒĞ±Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ñ Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
alias_map: dict[str, str] = {}

assign_re = re.compile(
    r"^\(([^)]+)\)\s*=\s*\((.+)\)$",
    re.MULTILINE)

for m in assign_re.finditer(code):
    left = [v.strip() for v in m.group(1).split(",")]
    right = [s.strip() for s in m.group(2).split(",")]

    for obf, expr in zip(left, right):
        if not obf:   # Ğ¿ÑƒÑÑ‚Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ° Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹
            continue

        # __import__('pkg.sub')
        imp = re.match(r"__import__\(\s*'([^']+)'\s*\)", expr)
        if imp:
            alias_map[obf] = imp.group(1).split(".")[-1]
            continue

        # getattr(â€¦, 'Something')
        gat = re.search(r"getattr\([^,]+,\s*'([^']+)'\s*\)", expr)
        if gat:
            alias_map[obf] = gat.group(1)
            continue

# Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ÑƒÑÑ‰Ğ¸Ğµ / Ğ·Ğ°Ñ€ĞµĞ·ĞµÑ€Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ
alias_map = {k: v for k, v in alias_map.items()
             if v.isidentifier() and not keyword.iskeyword(v)}

# Ñ€ÑƒÑ‡Ğ½Ğ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ²
seen = {}
for k, v in list(alias_map.items()):
    if v in seen:
        alias_map[k] = f"{v}_{seen[v]}"
        seen[v] += 1
    else:
        seen[v] = 1

print(f"ğŸ” Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ¿ÑĞµĞ²Ğ´Ğ¾Ğ½Ğ¸Ğ¼Ğ¾Ğ²: {len(alias_map)}")

# â”€â”€ 2. Ñ‚Ğ¾ĞºĞµĞ½-rename â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
out_tokens = []
stream = io.StringIO(code)
for tok in tokenize.generate_tokens(stream.readline):
    tok_type, tok_str, *rest = tok
    if tok_type == tokenize.NAME and tok_str in alias_map:
        tok_str = alias_map[tok_str]
    out_tokens.append((tok_type, tok_str))

new_code = tokenize.untokenize(out_tokens)

DST.write_text(new_code, encoding="utf-8")
print(f"âœ… layer15.py ÑĞ¾Ğ·Ğ´Ğ°Ğ½ â€” Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹.")